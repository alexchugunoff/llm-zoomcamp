{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f25c4f2-ebea-41c4-87ea-eb723c72222c",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac620d42-ac6b-4177-a582-8b761dd3a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minsearch\n",
    "import requests\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "from mistralai import Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2c68a9-6efb-48ab-af7e-3b29af7f7ea0",
   "metadata": {},
   "source": [
    "# Part 0: Basic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6142f9-9c81-4626-bfce-f5c6eb66a116",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff6a95cc-2aa7-49b2-8673-fc49ad3eaeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "965b738a-886a-4b6f-821c-c896d724274d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.append.AppendableIndex at 0x7c46c4207860>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import AppendableIndex\n",
    "\n",
    "index = AppendableIndex(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "479fb367-2aef-404d-adef-01ab543691ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "        output_ids=True\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c578f9d-6302-4905-8bf2-d5cec7431c9c",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f50bd61f-b571-403a-b21f-15cb8c1aa09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0f063c-98d3-4d99-8756-9760f9a9484b",
   "metadata": {},
   "source": [
    "## The RAG flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f4ba937-68da-4a8c-884b-02adf795ef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "model = \"mistral-large-latest\"\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "def llm(prompt):\n",
    "    response = client.chat.complete(\n",
    "        model= model,\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    )    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651bff18-ab0a-4778-99db-2102c9168031",
   "metadata": {},
   "source": [
    "# Part 1: Agentic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58cc2b87-33e9-4a04-abfd-eaa585c9e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making RAG more agentic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad3277e-42f3-47c4-a1ad-08e4ae67a9a9",
   "metadata": {},
   "source": [
    "## Update Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c709242-f529-4349-85a8-7b13ecd7840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant.\n",
    "\n",
    "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
    "At the beginning the context is EMPTY.\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT> \n",
    "{context}\n",
    "</CONTEXT>\n",
    "\n",
    "If CONTEXT is EMPTY, you can use our FAQ database.\n",
    "In this case, use the following output template:\n",
    "\n",
    "{{\n",
    "\"action\": \"SEARCH\",\n",
    "\"reasoning\": \"<add your reasoning here>\"\n",
    "}}\n",
    "\n",
    "If you can answer the QUESTION using CONTEXT, use this template:\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"CONTEXT\"\n",
    "}}\n",
    "\n",
    "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"OWN_KNOWLEDGE\"\n",
    "}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d826d0-a976-4024-b498-eb409bc56bfd",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b17feac7-c81e-4882-931b-4ff28b3b08bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "At the beginning the context is EMPTY.\n",
      "\n",
      "<QUESTION>\n",
      "how do I run docker on gentoo?\n",
      "</QUESTION>\n",
      "\n",
      "<CONTEXT> \n",
      "EMPTY\n",
      "</CONTEXT>\n",
      "\n",
      "If CONTEXT is EMPTY, you can use our FAQ database.\n",
      "In this case, use the following output template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\"\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"To run Docker on Gentoo, you need to follow these general steps:\\n\\n1. **Install Docker**: You can install Docker using the Portage package manager. First, ensure your system is up-to-date:\\n```\\nsudo emerge --sync\\nsudo emerge --update --deep --newuse @world\\n```\\nThen, install Docker:\\n```\\nsudo emerge app-emulation/docker\\n```\\n2. **Start the Docker Service**: After installation, start the Docker service:\\n```\\nsudo systemctl start docker\\n```\\nTo enable Docker to start on boot, use:\\n```\\nsudo systemctl enable docker\\n```\\n3. **Run Docker Commands**: You can now run Docker commands. For example, to run a simple Docker container:\\n```\\nsudo docker run hello-world\\n```\\nThis will download a test image and run it in a container. If the setup is correct, you should see a 'Hello from Docker!' message.\\n\\n4. **Add Your User to the Docker Group** (optional but recommended):\\n```\\nsudo usermod -aG docker $USER\\n```\\nAfter adding your user to the docker group, log out and log back in so that your group membership is re-evaluated. This allows you to run Docker commands without using sudo.\\n\\nThese steps should help you get Docker up and running on Gentoo. If you encounter any issues, make sure to check the Gentoo and Docker documentation for more detailed instructions.\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "question = \"how do I run docker on gentoo?\"\n",
    "context = \"EMPTY\"\n",
    "\n",
    "prompt = prompt_template.format(question=question, context=context)\n",
    "print(prompt)\n",
    "\n",
    "answer = llm(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31a02f9d-ce3d-4068-9c37-b9ab9d9e5f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"The CONTEXT is EMPTY, so I need to search the FAQ database to find the information on how to join the course.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "question = \"how do I join the course?\"\n",
    "context = \"EMPTY\"\n",
    "\n",
    "prompt = prompt_template.format(question=question, context=context)\n",
    "answer = llm(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deaff99-4b9c-4670-ab8b-2406d53176cd",
   "metadata": {},
   "source": [
    "## Implement make the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623430ca-3e25-4da1-aea1-4cd85287ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, build_context is a helper function from the previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57d0d4de-f382-4165-b724-dc93ef5490f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(search_results):\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "\n",
    "    return context.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d1bc41b-8b95-4a54-88eb-3a285af90813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "At the beginning the context is EMPTY.\n",
      "\n",
      "<QUESTION>\n",
      "how do I join the course?\n",
      "</QUESTION>\n",
      "\n",
      "<CONTEXT> \n",
      "section: General course-related questions\n",
      "question: Course - Can I still join the course after the start date?\n",
      "answer: Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don’t forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - Can I follow the course after it finishes?\n",
      "answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\n",
      "\n",
      "section: General course-related questions\n",
      "question: How do I use Git / GitHub for this course?\n",
      "answer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\n",
      "Having this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\n",
      "You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\n",
      "Remember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\n",
      "This is also a great resource: https://dangitgit.com/\n",
      "</CONTEXT>\n",
      "\n",
      "If CONTEXT is EMPTY, you can use our FAQ database.\n",
      "In this case, use the following output template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\"\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "search_results = search(question)\n",
    "context = build_context(search_results)\n",
    "prompt = prompt_template.format(question=question, context=context)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0eb0e4ce-c569-49f6-8244-0e98a24ca63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"To join the course, you need to register before it starts using this link. Additionally, join the course Telegram channel for announcements and register in DataTalks.Club's Slack to join the relevant channel.\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Check once again\n",
    "answer = llm(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92daa29d-ac83-4aee-8543-eda842a9bd83",
   "metadata": {},
   "source": [
    "## Put this together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa285312-b94b-4908-9274-0c6762f03217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First attempt to answer it with our know knowledge\n",
    "# If needed, do the lookup and then answer\n",
    "def agentic_rag_v1(question):\n",
    "    context = \"EMPTY\"\n",
    "    prompt = prompt_template.format(question=question, context=context)\n",
    "    answer_json = llm(prompt)\n",
    "    answer = json.loads(answer_json)\n",
    "    print(answer)\n",
    "\n",
    "    if answer['action'] == 'SEARCH':\n",
    "        print('need to perform search...')\n",
    "        search_results = search(question)\n",
    "        context = build_context(search_results)\n",
    "        \n",
    "        prompt = prompt_template.format(question=question, context=context)\n",
    "        answer_json = llm(prompt)\n",
    "        answer = json.loads(answer_json)\n",
    "        print(answer)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0121df5a-6555-4751-9133-8b3e2f462ea1",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27690b37-a377-4063-916a-e8b62c7811e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agentic_rag_v1('how do I join the course?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00027513-e48b-4b6d-971c-93c8306fe5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agentic_rag_v1('how patch KDE under FreeBSD?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e137ff66-06c0-41e6-b32f-b8d2225f738b",
   "metadata": {},
   "source": [
    "# Part 2: Agentic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c13efa-968b-4639-b4ca-c6039de93a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can let our \"agent\" formulate one or more search queries - and do it for a few iterations until we found an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1282bf24-c4c0-48fe-b057-a52f6f19d02a",
   "metadata": {},
   "source": [
    "## Update Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374280d1-4f58-4b30-9897-e879d9f49109",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant.\n",
    "\n",
    "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
    "\n",
    "The CONTEXT is build with the documents from our FAQ database.\n",
    "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
    "from FAQ to and add them to the context.\n",
    "PREVIOUS_ACTIONS contains the actions you already performed.\n",
    "\n",
    "At the beginning the CONTEXT is empty.\n",
    "\n",
    "You can perform the following actions:\n",
    "\n",
    "- Search in the FAQ database to get more data for the CONTEXT\n",
    "- Answer the question using the CONTEXT\n",
    "- Answer the question using your own knowledge\n",
    "\n",
    "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
    "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
    "\n",
    "Don't use search queries used at the previous iterations.\n",
    "\n",
    "Don't repeat previously performed actions.\n",
    "\n",
    "Don't perform more than {max_iterations} iterations for a given student question.\n",
    "The current iteration number: {iteration_number}. If we exceed the allowed number \n",
    "of iterations, give the best possible answer with the provided information.\n",
    "\n",
    "Output templates:\n",
    "\n",
    "If you want to perform search, use this template:\n",
    "\n",
    "{{\n",
    "\"action\": \"SEARCH\",\n",
    "\"reasoning\": \"<add your reasoning here>\",\n",
    "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
    "}}\n",
    "\n",
    "If you can answer the QUESTION using CONTEXT, use this template:\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER_CONTEXT\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"CONTEXT\"\n",
    "}}\n",
    "\n",
    "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"OWN_KNOWLEDGE\"\n",
    "}}\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<SEARCH_QUERIES>\n",
    "{search_queries}\n",
    "</SEARCH_QUERIES>\n",
    "\n",
    "<CONTEXT> \n",
    "{context}\n",
    "</CONTEXT>\n",
    "\n",
    "<PREVIOUS_ACTIONS>\n",
    "{previous_actions}\n",
    "</PREVIOUS_ACTIONS>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5e69ef-f4c4-4524-b08e-ee9258ded7e7",
   "metadata": {},
   "source": [
    "## First iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12801c77-0656-4c41-a47d-e8f95e8615d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the first iteration, we have this\n",
    "question = \"how do I join the course?\"\n",
    "\n",
    "search_queries = []\n",
    "search_results = []\n",
    "previous_actions = []\n",
    "context = build_context(search_results)\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    question=question,\n",
    "    context=context,\n",
    "    search_queries=\"\\n\".join(search_queries),\n",
    "    previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "    max_iterations=3,\n",
    "    iteration_number=1\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f6e0a-a473-4986-b587-5a95365aae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_json = llm(prompt)\n",
    "answer = json.loads(answer_json)\n",
    "print(json.dumps(answer, indent=2))\n",
    "\n",
    "# {\n",
    "#   \"action\": \"SEARCH\",\n",
    "#   \"reasoning\": \"I need to find specific information on how to join the course, as this information is not present in the current CONTEXT.\",\n",
    "#   \"keywords\": [\n",
    "#     \"how to join the course\",\n",
    "#     \"course enrollment process\",\n",
    "#     \"register for the course\"\n",
    "#   ]\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dcb187-dd8b-447e-815f-f9d6ee7a334b",
   "metadata": {},
   "source": [
    "## Save history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a51e0-bb12-410e-a845-5cf6d949414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to save the actions\n",
    "previous_actions.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f94553-20b0-4d75-bd0d-8c71af32ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the search queries\n",
    "keywords = answer['keywords']\n",
    "search_queries.extend(keywords)\n",
    "\n",
    "# And the search results\n",
    "for k in keywords:\n",
    "    res = search(k)\n",
    "    search_results.extend(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20156e1f-4603-4e00-bfcd-1b3e9dc2b534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the search results will be duplicates, so we need to remove them\n",
    "def dedup(seq):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for el in seq:\n",
    "        _id = el['_id']\n",
    "        if _id in seen:\n",
    "            continue\n",
    "        seen.add(_id)\n",
    "        result.append(el)\n",
    "    return result\n",
    "\n",
    "search_results = dedup(search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b70074-2dc7-4e92-9c4e-91d67ccf5853",
   "metadata": {},
   "source": [
    "## Another iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc71a17-0856-4bf0-93ef-9d76a66ea915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's make another iteration - use the same code as previously, \n",
    "# but remove variable initialization and increase the iteration number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534e26bf-0c5c-4a50-9270-c6f6d71a364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"how do I join the course?\"\n",
    "\n",
    "# search_queries = []\n",
    "# search_results = []\n",
    "# previous_actions = []\n",
    "\n",
    "context = build_context(search_results)\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    question=question,\n",
    "    context=context,\n",
    "    search_queries=\"\\n\".join(search_queries),\n",
    "    previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "    max_iterations=3,\n",
    "    iteration_number=2\n",
    ")\n",
    "print(prompt)\n",
    "\n",
    "answer_json = llm(prompt)\n",
    "answer = json.loads(answer_json)\n",
    "print(json.dumps(answer, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6acf70-a2f7-4ea1-b91d-51f9d1a2840f",
   "metadata": {},
   "source": [
    "## Put everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f91eaa-ebcd-441b-b429-7465c7365c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agentic_search(question):\n",
    "    search_queries = []\n",
    "    search_results = []\n",
    "    previous_actions = []\n",
    "\n",
    "    iteration = 0\n",
    "    \n",
    "    while True:\n",
    "        print(f'ITERATION #{iteration}...')\n",
    "    \n",
    "        context = build_context(search_results)\n",
    "        prompt = prompt_template.format(\n",
    "            question=question,\n",
    "            context=context,\n",
    "            search_queries=\"\\n\".join(search_queries),\n",
    "            previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "            max_iterations=3,\n",
    "            iteration_number=iteration\n",
    "        )\n",
    "    \n",
    "        print(prompt)\n",
    "    \n",
    "        answer_json = llm(prompt)\n",
    "        answer = json.loads(answer_json)\n",
    "        print(json.dumps(answer, indent=2))\n",
    "\n",
    "        previous_actions.append(answer)\n",
    "    \n",
    "        action = answer['action']\n",
    "        if action != 'SEARCH':\n",
    "            break\n",
    "    \n",
    "        keywords = answer['keywords']\n",
    "        search_queries = list(set(search_queries) | set(keywords))\n",
    "\n",
    "        for k in keywords:\n",
    "            res = search(k)\n",
    "            search_results.extend(res)\n",
    "    \n",
    "        search_results = dedup(search_results)\n",
    "        \n",
    "        iteration = iteration + 1\n",
    "        if iteration >= 4:\n",
    "            break\n",
    "    \n",
    "        print()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5865af-5c1f-4c4f-83d3-08e9c2ffec04",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38af7f83-edc2-4054-a8f5-8615a8446ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "agentic_search('how do I prepare for the course?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e566bb3-bce1-4ca8-81f8-a6bc550b3290",
   "metadata": {},
   "source": [
    "# Part 3: Function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5f1513-abef-4a0c-9ae3-35870e801a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We put all this logic inside our prompt.\n",
    "# But OpenAI and other providers provide a convenient API for adding extra functionality like search.\n",
    "# https://platform.openai.com/docs/guides/function-calling\n",
    "# It's called \"function calling\" - you define functions that the model can call, and if it decides to make a call, it returns structured output for that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c3872e-f0ca-4cdc-bfa8-6bba1477da4a",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2a0843-af35-4004-80b8-1674b14cbce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, let's take our search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1e1f58d-93f4-43e7-a3ee-fb3c2b630f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "        output_ids=True\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c7def1a-55fe-406e-9485-843873a9da5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We describe it like that for OpenAi\n",
    "search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"search\",\n",
    "    \"description\": \"Search the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48075a5c-7d32-486a-bf02-9855b1a8a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have:\n",
    "\n",
    "# name: search\n",
    "# description: when to use it\n",
    "# parameters: all the arguments that the function can take and their description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46a6ac17-b352-4852-bfa0-ddf0f024400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We describe it like that for Mistral\n",
    "search_tool_mistral = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"search\",\n",
    "        \"description\": \"Search the FAQ database\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fdae60-1422-44e5-a23c-c5105528ab84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5816cb91-c0ec-4d32-90d6-bae1abe40987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to use function calling, we'll use a newer API - the \"responses\" API (not \"chat completions\" as previously) for OpenAI\n",
    "question = \"How do I do well in module 1?\"\n",
    "\n",
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\"\"\".strip()\n",
    "\n",
    "tools = [search_tool]\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=model,\n",
    "    input=chat_messages,\n",
    "    tools=tools\n",
    ")\n",
    "response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ca03a65-b030-4964-8a0f-571b759f15cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Unmarshaller\nbody.0\n  Input tag 'developer' found using <lambda>() does not match any of the expected tags: 'assistant', 'system', 'tool', 'user' [type=union_tag_invalid, input_value={'role': 'developer', 'co... task is to answer it.\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/union_tag_invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      9\u001b[39m tools = [search_tool]\n\u001b[32m     11\u001b[39m chat_messages = [\n\u001b[32m     12\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mdeveloper\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: developer_prompt},\n\u001b[32m     13\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: question}\n\u001b[32m     14\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_tool_mistral\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43many\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m response.output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/mistralai/chat.py:174\u001b[39m, in \u001b[36mChat.complete\u001b[39m\u001b[34m(self, model, messages, temperature, top_p, max_tokens, stream, stop, random_seed, response_format, tools, tool_choice, presence_penalty, frequency_penalty, n, prediction, parallel_tool_calls, safe_prompt, retries, server_url, timeout_ms, http_headers)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    164\u001b[39m     base_url = \u001b[38;5;28mself\u001b[39m._get_url(base_url, url_variables)\n\u001b[32m    166\u001b[39m request = models.ChatCompletionRequest(\n\u001b[32m    167\u001b[39m     model=model,\n\u001b[32m    168\u001b[39m     temperature=temperature,\n\u001b[32m    169\u001b[39m     top_p=top_p,\n\u001b[32m    170\u001b[39m     max_tokens=max_tokens,\n\u001b[32m    171\u001b[39m     stream=stream,\n\u001b[32m    172\u001b[39m     stop=stop,\n\u001b[32m    173\u001b[39m     random_seed=random_seed,\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     messages=\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_pydantic_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mList\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    175\u001b[39m     response_format=utils.get_pydantic_model(\n\u001b[32m    176\u001b[39m         response_format, Optional[models.ResponseFormat]\n\u001b[32m    177\u001b[39m     ),\n\u001b[32m    178\u001b[39m     tools=utils.get_pydantic_model(tools, OptionalNullable[List[models.Tool]]),\n\u001b[32m    179\u001b[39m     tool_choice=utils.get_pydantic_model(\n\u001b[32m    180\u001b[39m         tool_choice, Optional[models.ChatCompletionRequestToolChoice]\n\u001b[32m    181\u001b[39m     ),\n\u001b[32m    182\u001b[39m     presence_penalty=presence_penalty,\n\u001b[32m    183\u001b[39m     frequency_penalty=frequency_penalty,\n\u001b[32m    184\u001b[39m     n=n,\n\u001b[32m    185\u001b[39m     prediction=utils.get_pydantic_model(\n\u001b[32m    186\u001b[39m         prediction, Optional[models.Prediction]\n\u001b[32m    187\u001b[39m     ),\n\u001b[32m    188\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    189\u001b[39m     safe_prompt=safe_prompt,\n\u001b[32m    190\u001b[39m )\n\u001b[32m    192\u001b[39m req = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m    193\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    194\u001b[39m     path=\u001b[33m\"\u001b[39m\u001b[33m/v1/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    208\u001b[39m     timeout_ms=timeout_ms,\n\u001b[32m    209\u001b[39m )\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retries == UNSET:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/mistralai/utils/serializers.py:206\u001b[39m, in \u001b[36mget_pydantic_model\u001b[39m\u001b[34m(data, typ)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_pydantic_model\u001b[39m(data: Any, typ: Any) -> Any:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _contains_pydantic_model(data):\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munmarshal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/mistralai/utils/serializers.py:147\u001b[39m, in \u001b[36munmarshal\u001b[39m\u001b[34m(val, typ)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munmarshal\u001b[39m(val, typ: Any) -> Any:\n\u001b[32m    141\u001b[39m     unmarshaller = create_model(\n\u001b[32m    142\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnmarshaller\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    143\u001b[39m         body=(typ, ...),\n\u001b[32m    144\u001b[39m         __config__=ConfigDict(populate_by_name=\u001b[38;5;28;01mTrue\u001b[39;00m, arbitrary_types_allowed=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m    145\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     m = \u001b[43munmarshaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m     \u001b[38;5;66;03m# pyright: ignore[reportAttributeAccessIssue]\u001b[39;00m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m m.body\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pydantic/main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for Unmarshaller\nbody.0\n  Input tag 'developer' found using <lambda>() does not match any of the expected tags: 'assistant', 'system', 'tool', 'user' [type=union_tag_invalid, input_value={'role': 'developer', 'co... task is to answer it.\"}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/union_tag_invalid"
     ]
    }
   ],
   "source": [
    "# In order to use function calling, we'll use a newer API - the \"responses\" API (not \"chat completions\" as previously) for Mistral\n",
    "question = \"How do I do well in module 1?\"\n",
    "\n",
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\"\"\".strip()\n",
    "\n",
    "tools = [search_tool]\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "response = client.chat.complete(\n",
    "    model = model,\n",
    "    messages = chat_messages,\n",
    "    tools = search_tool_mistral,\n",
    "    tool_choice = \"any\",\n",
    "    parallel_tool_calls = False,\n",
    ")\n",
    "response.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabce6cf-6522-4fc2-9294-5e651e5f1c2b",
   "metadata": {},
   "source": [
    "## Call to search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b707ee-b3d3-4635-89db-5113dedb188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mistral-large-latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721a6063-8c52-45b2-8aed-57311d50f9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "calls = response.output\n",
    "call = calls[0]\n",
    "call\n",
    "\n",
    "call_id = call.call_id\n",
    "call_id\n",
    "\n",
    "f_name = call.name\n",
    "f_name\n",
    "\n",
    "arguments = json.loads(call.arguments)\n",
    "arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a2d97-2856-4e67-a5ec-d6ac530cb43d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
